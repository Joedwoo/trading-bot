"""
Script for training Stock Trading Bot.

Usage:
  train.py <data-dir> [--strategy=<strategy>]
    [--window-size=<window-size>] [--batch-size=<batch-size>]
    [--episode-count=<episode-count>] [--model-name=<model-name>]
    [--patience=<patience>] [--pretrained] [--debug]

Options:
  --strategy=<strategy>             Q-learning strategy to use for training the network. Options:
                                      `dqn` i.e. Vanilla DQN,
                                      `t-dqn` i.e. DQN with fixed target distribution,
                                      `double-dqn` i.e. DQN with separate network for value estimation. [default: t-dqn]
  --window-size=<window-size>       Size of the n-day window stock data representation
                                    used as the feature vector. [default: 10]
  --batch-size=<batch-size>         Number of samples to train on in one mini-batch
                                    during training. [default: 32]
  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]
  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]
  --patience=<patience>             Number of episodes to wait for improvement before early stopping. [default: 3]
  --pretrained                      Specifies whether to continue training a previously
                                    trained model (reads `model-name`).
  --debug                           Specifies whether to use verbose logs during eval operation.
"""

import logging
import coloredlogs

from docopt import docopt

from trading_bot.agent import Agent
from trading_bot.methods import train_model, evaluate_model
from trading_bot.utils import (
    load_prepared_data,
    format_currency,
    format_position,
    show_train_result,
    switch_k_backend_device
)


def main(data_dir, window_size, batch_size, ep_count, patience=3,
         strategy="t-dqn", model_name="model_debug", pretrained=False,
         debug=False):
    """ Trains the stock trading bot using Deep Q-Learning.
    Please see https://arxiv.org/abs/1312.5602 for more details.

    Args: [python train.py --help]
    """
    # Charger les donn√©es pr√©-splitt√©es
    logging.info(f"Chargement des donn√©es pr√©-splitt√©es depuis {data_dir}")
    train_data, val_data, test_data = load_prepared_data(data_dir)
    
    # Calculer le nombre de features pour l'agent
    n_features = train_data['features'].shape[1]
    logging.info(f"Nombre de features: {n_features}")
    logging.info(f"Taille de l'√©tat: {(window_size - 1) + n_features}")
    
    # Initialiser l'agent avec la nouvelle signature
    agent = Agent(window_size, n_features, strategy=strategy, 
                  pretrained=pretrained, model_name=model_name)
    
    initial_offset = val_data['prices'][1] - val_data['prices'][0]
    
    # Variables pour early stopping et meilleur mod√®le
    best_val_profit = -float('inf')
    patience_counter = 0
    best_episode = 0
    
    logging.info("üöÄ D√©but de l'entra√Ænement...")
    logging.info(f"  - Strat√©gie: {strategy}")
    logging.info(f"  - Episodes: {ep_count}")
    logging.info(f"  - Window size: {window_size}")
    logging.info(f"  - Batch size: {batch_size}")
    logging.info(f"  - Mod√®le: {model_name}")
    logging.info(f"  - Patience: {patience} √©pisodes")

    for episode in range(1, ep_count + 1):
        train_result = train_model(agent, episode, train_data, ep_count=ep_count,
                                   batch_size=batch_size, window_size=window_size)
        val_result, _ = evaluate_model(agent, val_data, window_size, debug)
        show_train_result(train_result, val_result, initial_offset)
        
        # V√©rifier si c'est le meilleur mod√®le
        if val_result > best_val_profit:
            best_val_profit = val_result
            best_episode = episode
            patience_counter = 0
            
            # Sauvegarder le meilleur mod√®le
            agent.save_best()
            logging.info(f"üèÜ Nouveau meilleur r√©sultat de validation: ${best_val_profit:.2f} (Episode {episode})")
        else:
            patience_counter += 1
            logging.info(f"‚è≥ Pas d'am√©lioration depuis {patience_counter} √©pisode(s)")
            
            # Early stopping si patience d√©pass√©e
            if patience_counter >= patience:
                logging.info(f"üõë Early stopping d√©clench√© apr√®s {patience} √©pisodes sans am√©lioration")
                logging.info(f"üèÜ Meilleur r√©sultat: ${best_val_profit:.2f} (Episode {best_episode})")
                break
        
        # Sauvegarde p√©riodique (tous les 10 √©pisodes)
        if episode % 10 == 0:
            agent.save(episode)
    
    logging.info("‚úÖ Entra√Ænement termin√© !")
    logging.info(f"üèÜ Meilleur r√©sultat de validation: ${best_val_profit:.2f} (Episode {best_episode})")
    logging.info(f"üíæ Meilleur mod√®le: models/{model_name}_best")
    logging.info(f"üìÅ Mod√®les sauvegard√©s: models/{model_name}_*")


if __name__ == "__main__":
    args = docopt(__doc__)

    data_dir = args["<data-dir>"]
    strategy = args["--strategy"]
    window_size = int(args["--window-size"])
    batch_size = int(args["--batch-size"])
    ep_count = int(args["--episode-count"])
    model_name = args["--model-name"]
    patience = int(args["--patience"])
    pretrained = args["--pretrained"]
    debug = args["--debug"]

    coloredlogs.install(level="DEBUG")
    switch_k_backend_device()

    try:
        main(data_dir, window_size, batch_size, ep_count, patience,
             strategy=strategy, model_name=model_name, 
             pretrained=pretrained, debug=debug)
    except KeyboardInterrupt:
        print("Aborted!")
    except Exception as e:
        logging.error(f"Erreur: {e}")
